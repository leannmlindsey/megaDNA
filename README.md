# megaDNA: a long-context language model for deciphering and generating bacteriophage genomes
![Online_figure](https://github.com/lingxusb/megaDNA/assets/12596418/e2c2d658-758b-4b95-84a6-2126422c05ce)


Generative pre-trained transformers (GPTs) have revolutionized the field of natural language processing. Inspired by the success of large language models, we develop a long-context generative model for genomes. Our multiscale transformer model was pre-trained on unannotated bacteriophage genomes with byte-level tokenization. We demonstrate the foundational capabilities of our model including the prediction of essential genes, genetic variant effects, regulatory element activity and taxonomy of unannotated sequences. Furthermore, it generates de novo sequences up to 96K base pairs, which contain functional regulatory elements and novel proteins with phage-related functions.

### Install
To install `megaDNA`, run the following bash script:
 ```bash
 git clone https://github.com/lingxusb/megaDNA.git
 cd megaDNA
 pip install .
 ```

### Trained model
- The original model: [megaDNA_145M](https://huggingface.co/lingxusb/megaDNA_updated/tree/main).
- Other model sizes: [megaDNA_78M and megaDNA_277M](https://huggingface.co/lingxusb/megaDNA_variants).
- Fine-tuned model on E. coli phage: [megaDNA_ecoli](https://huggingface.co/lingxusb/megaDNA_finetuned).

### Sequence generation
```python
 import torch
 
 # load the model

 device = 'cpu' # use 'cuda' for GPU
 model = torch.load(model_path, map_location=torch.device(device))

 # new sequences can be generated by a primer sequence:

 nucleotides = ['**', 'A', 'T', 'C', 'G', '#'] # vocabulary

 seq_tokenized = model.generate(primer_sequence,
                                seq_len=context_length,
                                temperature=0.95, 
                                filter_thres=0.0)

 # To transform tokens back to DNA ucleotide sequence:
 def token2nucleotide(s):
     return nucleotides[s]
 generated_sequence = ''.join(map(token2nucleotide, seq_tokenized.squeeze().cpu().int()))
 ```

Please check our jupyter notebook: [megaDNA_generate.ipynb](https://github.com/lingxusb/megaDNA/blob/main/notebook/megaDNA_generate.ipynb). GPU recommended.

Or you can easily run the [Colab Notebook](https://colab.research.google.com/drive/13C9uyKfziydSiWGyD3nxacx0-zG_irqV?usp=sharing) in the browser. Please make sure to connect to a GPU instance (e.g. T4 GPU).

**Features for the generated sequences**
- Annotated genes (Fig. 2b)
- Annotated proteins with diverse functions (Fig. 2i & Fig. S12)
- Folding of annotated proteins (Fig. 2h & Fig. S11)
- Virus score that is comparable with natural phages (Fig. 2c)
- Marker genes for phage (Fig. 2h)
- Classified as Caudoviricetes (~37%, Fig. 2d)
- Predicted hosts (~40%, Fig. S9)
- Regulatory elements  including promoters and RBS (Fig. 2f, 2g, Fig. S10)

Please check our [preprint](https://www.biorxiv.org/content/10.1101/2023.12.18.572218v3) for more details.


### Model embeddings and loss
```python
# a random input sequence
encoded_sequence = np.random.choice(np.arange(1,5), 100)
input_seq = torch.tensor(encoded_sequence).unsqueeze(0).to(device) 

# get embeddings
output = model(input_seq, return_value = 'embedding')

# output[0:3] stores embeddings from three transformer layers.

# get model loss
output = model(input_seq, return_value = 'loss')

print(output)
```

### In silico mutagenesis analysis
Please check our jupyter notebook: [megaDNA_mutagenesis.ipynb](https://github.com/lingxusb/megaDNA/blob/main/notebook/megaDNA_mutagenesis.ipynb). Fasta file and gene annotation for lambda phage can be downloaded from https://www.ncbi.nlm.nih.gov/nuccore/NC_001416.1

---

## Binary Classification Pipeline

megaDNA is a causal (generative) model, so instead of fine-tuning, classification is done by:
1. Extracting embeddings from the pretrained backbone
2. Training a lightweight classifier (linear probe + 3-layer NN) on those embeddings
3. Running inference with the saved classifier

### Prepare Your Data

Create a directory containing three CSV files with `sequence` and `label` columns:

```
my_dataset/
├── train.csv
├── dev.csv    # (or val.csv)
└── test.csv
```

Each CSV should have this format:
```csv
sequence,label
ACGTACGTACGT...,0
TGCATGCATGCA...,1
```

### Embedding Layers

megaDNA has three transformer layers capturing different context lengths:

| Layer | Dimensions | Context |
|-------|:----------:|---------|
| `local` | 196 | 16 bp |
| `middle` | 256 | 1024 bp |
| `global` | 512 | 96K bp |
| `all` | 964 | All concatenated |

---

### Step 1: Embedding Analysis (train classifiers)

Extract embeddings, train a linear probe and a 3-layer NN, and evaluate embedding quality.

```bash
python embedding_analysis_megadna.py \
    --csv_dir /path/to/csv/data \
    --model_path /path/to/megaDNA_phage_145M.pt \
    --output_dir ./results/embedding_analysis \
    --layer middle \
    --pooling mean
```

**All options:**

| Argument | Default | Description |
| -------- | ------- | ----------- |
| `--csv_dir` | (required) | Directory containing train.csv, dev.csv/val.csv, test.csv |
| `--model_path` | (required) | Path to pretrained megaDNA checkpoint (.pt file) |
| `--output_dir` | `./results/embedding_analysis` | Base output directory |
| `--layer` | `middle` | Embedding layer: `local`, `middle`, `global`, or `all` |
| `--pooling` | `mean` | Pooling strategy: `mean`, `max`, or `cls` |
| `--batch_size` | 8 | Batch size for embedding extraction |
| `--max_length` | 96000 | Max sequence length in bp |
| `--nn_epochs` | 100 | Training epochs for the 3-layer NN |
| `--nn_hidden_dim` | 256 | Hidden dimension for the 3-layer NN |
| `--nn_lr` | 0.001 | Learning rate for the 3-layer NN |
| `--seed` | 42 | Random seed |
| `--include_random_baseline` | off | Also evaluate a randomly initialized model for comparison |

**Outputs (saved to `output_dir`):**

| File | Description |
| ---- | ----------- |
| `embeddings_pretrained.npz` | Cached embeddings (reused on re-run) |
| `three_layer_nn_pretrained.pt` | Trained 3-layer NN checkpoint |
| `three_layer_nn_pretrained_scaler.pkl` | StandardScaler used to normalize embeddings |
| `test_predictions_pretrained.csv` | Test set predictions from both classifiers |
| `pca_visualization_pretrained.png` | PCA plot showing class separation |
| `embedding_analysis_results.json` | All metrics (accuracy, F1, MCC, AUC, silhouette, etc.) |

To compare against a randomly initialized baseline:

```bash
python embedding_analysis_megadna.py \
    --csv_dir /path/to/csv/data \
    --model_path /path/to/megaDNA_phage_145M.pt \
    --include_random_baseline
```

---

### Step 2: Inference (classify new sequences)

Use the trained 3-layer NN classifier from Step 1 to classify new sequences. This requires three files from the embedding analysis output:
- The megaDNA backbone (`--model_path`)
- The trained NN classifier (`--classifier_path`)
- The saved scaler (`--scaler_path`)

**Important:** `--layer` and `--pooling` must match the values used in Step 1.

```bash
python inference_megadna.py \
    --input_csv /path/to/test.csv \
    --model_path /path/to/megaDNA_phage_145M.pt \
    --classifier_path ./results/embedding_analysis/three_layer_nn_pretrained.pt \
    --scaler_path ./results/embedding_analysis/three_layer_nn_pretrained_scaler.pkl \
    --layer middle \
    --pooling mean \
    --save_metrics
```

**All options:**

| Argument | Default | Description |
| -------- | ------- | ----------- |
| `--input_csv` | (required) | CSV file with a `sequence` column (and optionally `label`) |
| `--model_path` | (required) | Path to pretrained megaDNA checkpoint (.pt file) |
| `--classifier_path` | (required) | Path to `three_layer_nn_pretrained.pt` from Step 1 |
| `--scaler_path` | (required) | Path to `three_layer_nn_pretrained_scaler.pkl` from Step 1 |
| `--output_csv` | auto | Output path (default: `<input>_predictions.csv`) |
| `--layer` | `middle` | Embedding layer (must match Step 1) |
| `--pooling` | `mean` | Pooling strategy (must match Step 1) |
| `--batch_size` | 8 | Batch size for embedding extraction |
| `--max_length` | 96000 | Max sequence length in bp |
| `--threshold` | 0.5 | Classification threshold for prob_1 |
| `--save_metrics` | off | Save metrics to JSON (requires `label` column in input) |

**Output CSV columns:**

| Column | Description |
| ------ | ----------- |
| `sequence` | Original DNA sequence |
| `label` | Original label (if present in input) |
| `prob_0` | Probability of class 0 |
| `prob_1` | Probability of class 1 |
| `pred_label` | Predicted label |

If `--save_metrics` is used and labels are present, a `_metrics.json` file is also saved with accuracy, precision, recall, F1, MCC, AUC, sensitivity, and specificity.

---

### SLURM Scripts (for HPC)

SLURM scripts are provided in `slurm_scripts/` for running on HPC clusters:

```bash
# Embedding analysis
# 1. Edit configuration in slurm_scripts/wrapper_run_embedding_analysis.sh
# 2. Submit:
bash slurm_scripts/wrapper_run_embedding_analysis.sh
# Or run interactively:
bash slurm_scripts/run_embedding_analysis_interactive.sh

# Inference
# 1. Edit configuration in slurm_scripts/wrapper_run_inference.sh
# 2. Submit:
bash slurm_scripts/wrapper_run_inference.sh
# Or run interactively:
bash slurm_scripts/run_inference_interactive.sh
```

---

### Reference
- [A long-context language model for deciphering and generating bacteriophage genomes](https://www.biorxiv.org/content/10.1101/2023.12.18.572218v3)
- [MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/abs/2305.07185)
- [MEGABYTE-pytorch by Phil Wang](https://github.com/lucidrains/MEGABYTE-pytorch)
- [Protein language models learn evolutionary statistics of interacting sequence motifs](https://www.biorxiv.org/content/10.1101/2024.01.30.577970v1)
- Please contact shaobinlx@gmail.com or raise an issue in the github repo with any questions.
